梯度下降算法的局限性
- **初值敏感**，变量越多越敏感，不同的初值可能收敛到不同的极值点
- **学习率敏感**，可动态改变学习率
- **无法完全保证取得的是全局最值**

![](../../photo/Pasted%20image%2020240531153210.png)

![](../../photo/Pasted%20image%2020240531153159.png)
![](../../photo/Pasted%20image%2020240531162754.png)

![](../../photo/Pasted%20image%2020240531161746.png)
# 算法
```
# 用梯度下降算法求函数辰小值：L=x1^2+3*x2^2
# 越接近极值点，梯度变化越小
# 当x1 x2 为多少时，有极小值
# 梯度下降算法，不断迭代，迭代差值不断变小（越接近极值点，差值变化越小，想象函数图像）

m=0.01  # 学习率
x1=-10  # 初始值
X2=30   # 初始值
L = x1**2 + 3*x2**2 # 函数方程
n=0     # 执行的次数
err=1   # 迭代前后的函数差值
threshold=0.0000001     # 迭代前后的函数差值，和极值的差值
value = []      # 函数差值的变化

while (err > threshold):
    x1=x1-2*m*x1    #迭代，梯度下降算法，当前值=旧值-学习率*导数
    x2=x2-6*m*x2    #达代，梯度下降算法，当前值=旧值-学习率*导数
    err=abs(x1^2+3*x2^2-L)  #计算前后两次送代后函数差的绝对信
    value.append(err)   # 打印迭代后函数差值的变化
    L = x1**2 + 3*x2**2
    n = n+1
print(x1,x2,L,n)
plt.plot(value)
plt.show()
```
![](../../photo/Pasted%20image%2020240531160755.png)
